{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of Members:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __APA, GIUSIPPI MARIA II DEL ROSARIO__\n",
    "- __OBEJERO, REY FERBS MAGALLON__\n",
    "- __RAMIREZ, BENMAR SIM GREFALDA__\n",
    "- __RAMOS, RONN PATRICK BICERA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we chose for this machine project is the \"Sports\" dataset. \n",
    "\n",
    "Our project will have us go through the process of selecting a dataset, describing it, performing EDA, data preprocessing and cleaning, model training, HP tuning, model selection, and extracting insights from the data.\n",
    "\n",
    "We aim to leverage the data from the soccer matches to preduct the number of goals that will be scored during the remaining playtime. The target of this project is regression as we aim to predict a continous outcome, which is the **number of goals to be scored** during the remaining time.\n",
    "\n",
    "We will implement the use of machine learning and other techniques taught in class to build our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is related to soccer matches and will be used to predict how many goals will be scored during the remaining match time. The dataset contains 10,000 such snapshots, each taken at a different, random point in time during a match. \n",
    "\n",
    "In the context of this dataset, a “snapshot” refers to a set of data points captured at a specific moment during a soccer match.\n",
    "\n",
    "As for the collection process, it can be inferred that the data might have been collected in real-time during this soccer match, capturing various events and statistics at different timestamps. This data would allow for more accurate predictions of future events based on the current state of the match, and whether it is best to bet on over or under a certain number of goals.\n",
    "\n",
    "There are only 7000 instances(rows) and 21 features(columns) in the dataset. Each of the features are statistics of both events for the home and away team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Variable Name** | **Description**|\n",
    "|--------------------------------------|----------------|\n",
    "|**uuid** | Snapshot’s unique ID|\n",
    "|**current_minute** | Match’s current minute, including 15 min half-time|\n",
    "|**home_score and away_score** | Home and away team’s goals.|\n",
    "|**home_yellow_cards and away_yellow_cards**| Home and away team’s yellow cards.|\n",
    "|**home_red_cards and away_red_cards**| Home and away team’s red cards.|\n",
    "|**home_attacks and away_attacks**| Home and away team’s attempted attacks.|\n",
    "|**home_dangerous_attacks and away_dangerous_attacks**| Home and away team’s dangerous attacks.|\n",
    "|**home_corners and away_corners**| Home and away team’s awarded corners.|\n",
    "|**home_off_target and away_off_target**| Home and away team’s off-target shots.|\n",
    "|**home_on_target and away_on_target**| Home and away team’s on-target shots.|\n",
    "|**home_possession and away_possession** |Home and away team’s ball possession %.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this machine project, we will utilize the following Python libraries:\n",
    "\n",
    "- `one`:\n",
    "- `two`:\n",
    "- `three`:\n",
    "- `four`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import h5py\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from linear_regression import AnalyticalMethod, compute_RMSE\n",
    "from linear_regression import PolyFitMethod\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from neural_network import NeuralNetwork\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_predict, cross_val_score, ParameterGrid\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be performing various methods of preprocessing and data cleaning to enhance the dataset's usability and suitability to our exploratory analysis and model training.\n",
    "\n",
    "This is also to ensure that the dataset will be ready for use, which means we will be checking for missing values, duplicates, outliers and other errors. Making it so that the data is in its correct representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and reading the Sports.csv\n",
    "sports = pd.read_csv(\"sports.csv\")\n",
    "sports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Missing Values**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column for missing values\n",
    "missing_values = sports.isnull().sum()\n",
    "\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Duplicates**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicates\n",
    "duplicate_uuids = sports.duplicated(keep=False)\n",
    "\n",
    "# Create a dataframe for duplicates\n",
    "duplicates = sports[duplicate_uuids]\n",
    "\n",
    "# Display the rows with duplicates\n",
    "print(\"Duplicate Entries:\\n\", duplicates)\n",
    "\n",
    "# Removing the duplicates if detected in dataframe\n",
    "if not duplicates.empty:\n",
    "    sports = sports.drop_duplicates(keep='first')\n",
    "    print(\"Duplicates removed. Dataset has {sports.shape[0]} rows.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Cleaning Processes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing any erroneous infinite values\n",
    "sports = sports.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Dropping columns with more than 50% missing values\n",
    "sports = sports.dropna(thresh=len(sports)*0.5, axis=1)\n",
    "\n",
    "# Check for incorrect possession percentages\n",
    "incorrect_possession_indices = sports[~(sports['home_possession'] + sports['away_possession'] == 100)].index\n",
    "\n",
    "if len(incorrect_possession_indices) > 0:\n",
    "    print(\"Found inconsistencies in possession data:\")\n",
    "    print(sports.loc[incorrect_possession_indices, ['home_possession', 'away_possession']])\n",
    "else:\n",
    "    print(\"Possession data is consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are percentages that don't equate to 100, but the difference is neglible because it is only from 99 to 100 which won't cause much noise in the analysis. It isn't exactly necessary to remove it at this point of the dataset but is also another consideration we took to prepare the if ever the dataset grew larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Type Checking**\n",
    "\n",
    "Having appropriate data types for the values in the dataset is crucial to data analysis and model training. Luckily for us, the dataset we currently have has the appropriate values as all of the variables and their values are appropriate to their data type.\n",
    "\n",
    "The importance of having correct data types is that we have the following:\n",
    "\n",
    "Data Integrity,\n",
    "Accuracy,\n",
    "Efficiency,\n",
    "Compatibility, \n",
    "and Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing all of the present data types\n",
    "print(\"All data types present:\\n\", sports.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and Treatment\n",
    "\n",
    "Outliers can significantly impact statistical measures and model performance. In the context of our dataset, outliers might represent unusual game conditions, errors in data collection, or truly exceptional events. \n",
    "\n",
    "Surprisingly enough, there are no major outliers in our dataset, because in the dataset, varying values like attacks, cards, and accuracy are all subject to the team's playstyle and ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we performed winsorization on the dataset to make it robust against potential outliers in the future if ever there are any in the future, possibly when the dataset grows larger. By capping the data at specified percentiles we reduce the possible influed of these outliers. \n",
    "\n",
    "We also provided the before and after outlier treatment visualizations to identify the outliers (if there are any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sports.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data from CSV\n",
    "sports = pd.read_csv('sports.csv')\n",
    "\n",
    "# Columns to check for outliers\n",
    "outlier_columns = ['current_minute', 'home_yellow_cards', 'away_yellow_cards', 'home_on_target',\n",
    "                   'away_on_target', 'home_attacks', 'away_attacks', 'home_dangerous_attacks',\n",
    "                   'away_dangerous_attacks', 'home_corners', 'away_corners', 'home_off_target',\n",
    "                   'away_off_target', 'home_possession', 'away_possession']\n",
    "\n",
    "def winsorize(data, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Winsorizes data by capping values at specified percentiles.\"\"\"\n",
    "    lower_bound = np.percentile(data, lower_percentile)\n",
    "    upper_bound = np.percentile(data, upper_percentile)\n",
    "    return np.clip(data, lower_bound, upper_bound)\n",
    "\n",
    "def truncate(data, upper_bound):\n",
    "    \"\"\"Truncates data by removing values exceeding the upper bound.\"\"\"\n",
    "    return data[data <= upper_bound]\n",
    "\n",
    "def detect_possession_outliers(data, method='zscore', z_score_threshold=3, lower_bound=20, upper_bound=80):\n",
    "    \"\"\"\n",
    "    Identifies outliers in home and away possession data using different methods.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas DataFrame containing home_possession and away_possession columns.\n",
    "        method: The outlier detection method to use ('zscore', 'percentile', 'domain').\n",
    "        z_score_threshold: Threshold for Z-scores (used with 'zscore' method).\n",
    "        lower_bound: Lower threshold for possession values (used with 'domain' method).\n",
    "        upper_bound: Upper threshold for possession values (used with 'domain' method).\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with additional columns indicating potential outliers.\n",
    "    \"\"\"\n",
    "    home_possession = data['home_possession']\n",
    "    away_possession = data['away_possession']\n",
    "\n",
    "    if method == 'zscore':\n",
    "        # Z-score method (not used for normalization)\n",
    "        pass\n",
    "    elif method == 'percentile':\n",
    "        # Percentile-based method (not used for normalization)\n",
    "        pass\n",
    "    elif method == 'domain':\n",
    "        # Domain knowledge-based method\n",
    "        data['home_possession_outlier'] = np.where((home_possession < lower_bound) | (home_possession > upper_bound), True, False)\n",
    "        data['away_possession_outlier'] = np.where((away_possession < lower_bound) | (away_possession > upper_bound), True, False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid outlier detection method. Choose 'zscore', 'percentile', or 'domain'.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Handle outliers for other columns\n",
    "for col in outlier_columns:\n",
    "    if col == 'current_minute':\n",
    "        sports[col] = truncate(sports[col], 105)\n",
    "    elif col.endswith('_cards'):\n",
    "        sports[col] = winsorize(sports[col], upper_percentile=80)\n",
    "    else:\n",
    "        sports[col] = winsorize(sports[col])\n",
    "\n",
    "# Choose a method for possession outliers and apply it\n",
    "sports = detect_possession_outliers(sports.copy(), method='domain', lower_bound=20, upper_bound=80)  # Example: using domain-based method\n",
    "\n",
    "# Calculate goals scored after current minute\n",
    "def calculate_remaining_goals(row):\n",
    "    total_goals = row['home_score'] + row['away_score']\n",
    "    goals_so_far = total_goals - row['home_score'] - row['away_score']\n",
    "    return total_goals - goals_so_far\n",
    "\n",
    "sports['goals_remaining'] = sports.apply(calculate_remaining_goals, axis=1)\n",
    "\n",
    "# Calculate additional features\n",
    "epsilon = 1e-10  # Add a small number to avoid division by zero\n",
    "sports['home_attack_efficiency'] = sports['home_score'] / (sports['home_attacks'] + epsilon)\n",
    "sports['away_attack_efficiency'] = sports['away_score'] / (sports['away_attacks'] + epsilon)\n",
    "\n",
    "sports['home_shot_conversion_rate'] = sports['home_score'] / (sports['home_on_target'] + sports['home_off_target'] + epsilon)\n",
    "sports['away_shot_conversion_rate'] = sports['away_score'] / (sports['away_on_target'] + sports['away_off_target'] + epsilon)\n",
    "\n",
    "sports['possession_differential'] = sports['home_possession'] - sports['away_possession']\n",
    "\n",
    "# Select columns for normalization (excluding uuid)\n",
    "columns_to_normalize = [col for col in sports.columns if col != 'uuid']\n",
    "\n",
    "# Create a MinMaxScaler object (normalizes to -1 to 1 range)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Apply normalization to selected columns\n",
    "sports[columns_to_normalize] = scaler.fit_transform(sports[columns_to_normalize])\n",
    "\n",
    "# Visualize distributions after outlier treatment and normalization\n",
    "fig, axes = plt.subplots(nrows=len(outlier_columns) + 2, ncols=2, figsize=(15, 40))\n",
    "\n",
    "for i, col in enumerate(outlier_columns):\n",
    "    # Box plots\n",
    "    sns.boxplot(data=sports, x=col, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'{col} - Before Treatment')\n",
    "\n",
    "    sns.boxplot(data=sports, x=col, ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f'{col} - After Treatment')\n",
    "\n",
    "# Visualize possession data with outliers\n",
    "sns.boxplot(data=sports, x='home_possession', ax=axes[len(outlier_columns), 0], hue='home_possession_outlier')\n",
    "sns.boxplot(data=sports, x='away_possession', ax=axes[len(outlier_columns), 1], hue='away_possession_outlier')\n",
    "\n",
    "# Histograms for possession data with outliers\n",
    "sns.histplot(data=sports, x='home_possession', kde=True, ax=axes[len(outlier_columns) + 1, 0], hue='home_possession_outlier')\n",
    "sns.histplot(data=sports, x='away_possession', kde=True, ax=axes[len(outlier_columns) + 1, 1], hue='away_possession_outlier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did boxplots and histograms for all relevant variables and utilized truncation and winsorization. \n",
    "\n",
    "In our current case, there are no obvious outliers, because the initial data might not have contained extreme values that would significantly impact the results. There is also the idea of playstyle variations where the teams at different current_minutes would have varying possessions, attacks, or accuracy.\n",
    "\n",
    "Despite this, still applying outlier detection methods still offers a safety net from future outliners. We also applied normalization to bring the features to a common scale between 0 and 1, this will help us ensure that there is feature scaling and improved algorithm performance from algorithms like such as neural networks and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "`goals_remaining`: This is our target variable, representing the number of goals scored in the remaining time of the match.\n",
    "\n",
    "Ratio of Attack to Goals\n",
    "\n",
    "`home_attack_efficiency`: Measures the efficiency of the home team in converting attacks into goals.\n",
    "`away_attack_efficiency`: Measures the efficiency of the away team in converting attacks into goals.\n",
    "\n",
    "Ratio of Shots to Goals\n",
    "\n",
    "`home_shot_conversion_rate`: Measures the efficiency of the home team in converting shots into goals.\n",
    "`away_shot_conversion_rate`: Measures the efficiency of the away team in converting shots into goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate goals scored after current minute\n",
    "def calculate_remaining_goals(row):\n",
    "    total_goals = row['home_score'] + row['away_score']\n",
    "    goals_so_far = total_goals - row['home_score'] - row['away_score']\n",
    "    return total_goals - goals_so_far\n",
    "\n",
    "sports['goals_remaining'] = sports.apply(calculate_remaining_goals, axis=1)\n",
    "\n",
    "# Calculate additional features\n",
    "sports['home_attack_efficiency'] = sports['home_score'] / sports['home_attacks']\n",
    "sports['away_attack_efficiency'] = sports['away_score'] / sports['away_attacks']\n",
    "sports['home_shot_conversion_rate'] = sports['home_score'] / (sports['home_on_target'] + sports['home_off_target'])\n",
    "sports['away_shot_conversion_rate'] = sports['away_score'] / (sports['away_on_target'] + sports['away_off_target'])\n",
    "\n",
    "print(sports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Features**\n",
    "\n",
    "The uuid column is a unique identifier that does not provide any predictive information, therefore it's correct to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant column\n",
    "sports = sports.drop('uuid', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# set default size of plots\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
