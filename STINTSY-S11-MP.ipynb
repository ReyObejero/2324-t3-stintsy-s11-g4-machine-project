{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of Members:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __APA, GIUSIPPI MARIA II DEL ROSARIO__\n",
    "- __OBEJERO, REY FERBS MAGALLON__\n",
    "- __RAMIREZ, BENMAR SIM GREFALDA__\n",
    "- __RAMOS, RONN PATRICK BICERA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we chose for this machine project is the \"Sports\" dataset. \n",
    "\n",
    "Our project will have us go through the process of selecting a dataset, describing it, performing EDA, data preprocessing and cleaning, model training, HP tuning, model selection, and extracting insights from the data.\n",
    "\n",
    "We aim to leverage the data from the soccer matches to preduct the number of goals that will be scored during the remaining playtime. The target of this project is regression as we aim to predict a continous outcome, which is the **number of goals to be scored** during the remaining time.\n",
    "\n",
    "We will implement the use of machine learning and other techniques taught in class to build our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is related to soccer matches and will be used to predict how many goals will be scored during the remaining match time. The dataset contains 10,000 such snapshots, each taken at a different, random point in time during a match. \n",
    "\n",
    "In the context of this dataset, a “snapshot” refers to a set of data points captured at a specific moment during a soccer match.\n",
    "\n",
    "As for the collection process, it can be inferred that the data might have been collected in real-time during this soccer match, capturing various events and statistics at different timestamps. This data would allow for more accurate predictions of future events based on the current state of the match, and whether it is best to bet on over or under a certain number of goals.\n",
    "\n",
    "There are only 7000 instances(rows) and 21 features(columns) in the dataset. Each of the features are statistics of both events for the home and away team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Variable Name** | **Description**|\n",
    "|--------------------------------------|----------------|\n",
    "|**uuid** | Snapshot’s unique ID|\n",
    "|**current_minute** | Match’s current minute, including 15 min half-time|\n",
    "|**home_score and away_score** | Home and away team’s goals.|\n",
    "|**home_yellow_cards and away_yellow_cards**| Home and away team’s yellow cards.|\n",
    "|**home_red_cards and away_red_cards**| Home and away team’s red cards.|\n",
    "|**home_attacks and away_attacks**| Home and away team’s attempted attacks.|\n",
    "|**home_dangerous_attacks and away_dangerous_attacks**| Home and away team’s dangerous attacks.|\n",
    "|**home_corners and away_corners**| Home and away team’s awarded corners.|\n",
    "|**home_off_target and away_off_target**| Home and away team’s off-target shots.|\n",
    "|**home_on_target and away_on_target**| Home and away team’s on-target shots.|\n",
    "|**home_possession and away_possession** |Home and away team’s ball possession %.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this machine project, we will utilize the following Python libraries:\n",
    "\n",
    "- `one`:\n",
    "- `two`:\n",
    "- `three`:\n",
    "- `four`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import h5py\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from neural_network import NeuralNetwork\n",
    "from linear_regression import AnalyticalMethod, compute_RMSE\n",
    "from linear_regression import PolyFitMethod\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_predict, cross_val_score, ParameterGrid\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be performing various methods of preprocessing and data cleaning to enhance the dataset's usability and suitability to our exploratory analysis and model training.\n",
    "\n",
    "This is also to ensure that the dataset will be ready for use, which means we will be checking for missing values, duplicates, outliers and other errors. Making it so that the data is in its correct representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and reading the Sports.csv\n",
    "sports = pd.read_csv(\"sports.csv\")\n",
    "sports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Missing Values**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code here indentifies and prints the columns in our sports Dataframe that contain missing values, first one counts the total number of missing values in each column, and if the count is greater than zero it shows us the list.\n",
    "\n",
    "This is an essential step, just so we can see if we have any columns that are missing data before we move on to other processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each column for missing values\n",
    "missing_values = sports.isnull().sum()\n",
    "\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Duplicates**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code identifies and removes duplicates rows in the sports DataFrame, it finds all the duplicate rows and seperates them and prints out the rows. Duplicates are removed from the original dataframe and will only keep the first occurence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicates\n",
    "duplicate_uuids = sports.duplicated(keep=False)\n",
    "\n",
    "# Create a dataframe for duplicates\n",
    "duplicates = sports[duplicate_uuids]\n",
    "\n",
    "# Display the rows with duplicates\n",
    "print(\"Duplicate Entries:\\n\", duplicates)\n",
    "\n",
    "# Removing the duplicates if detected in dataframe\n",
    "if not duplicates.empty:\n",
    "    sports = sports.drop_duplicates(keep='first')\n",
    "    print(\"Duplicates removed. Dataset has {sports.shape[0]} rows.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Cleaning Processes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first process replaces infinite values with NaN, which is for undefined data. Drops any columns where there are more than 50% values are missing, while the last checks for inconsistencies in the possession data, wherein it looks for home_possession and way_possession variables that do not equate to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing any erroneous infinite values\n",
    "sports = sports.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Dropping columns with more than 50% missing values\n",
    "sports = sports.dropna(thresh=len(sports)*0.5, axis=1)\n",
    "\n",
    "# Check for incorrect possession percentages\n",
    "incorrect_possession_indices = sports[~(sports['home_possession'] + sports['away_possession'] == 100)].index\n",
    "\n",
    "if len(incorrect_possession_indices) > 0:\n",
    "    print(\"Found inconsistencies in possession data:\")\n",
    "    print(sports.loc[incorrect_possession_indices, ['home_possession', 'away_possession']])\n",
    "else:\n",
    "    print(\"Possession data is consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are percentages that don't equate to 100, but the difference is neglible because it is only from 99 to 100 which won't cause much noise in the analysis. It isn't exactly necessary to remove it at this point of the dataset but is also another consideration we took to prepare the if ever the dataset grew larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Type Checking**\n",
    "\n",
    "Having appropriate data types for the values in the dataset is crucial to data analysis and model training. Luckily for us, the dataset we currently have has the appropriate values as all of the variables and their values are appropriate to their data type.\n",
    "\n",
    "The importance of having correct data types is that we have the following:\n",
    "\n",
    "Data Integrity,\n",
    "Accuracy,\n",
    "Efficiency,\n",
    "Compatibility, \n",
    "and Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing all of the present data types\n",
    "print(\"All data types present:\\n\", sports.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and Treatment\n",
    "\n",
    "Outliers can significantly impact statistical measures and model performance. In the context of our dataset, outliers might represent unusual game conditions, errors in data collection, or truly exceptional events. \n",
    "\n",
    "Surprisingly enough, there are no major outliers in our dataset, because in the dataset, varying values like attacks, cards, and accuracy are all subject to the team's playstyle and ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we performed winsorization on the dataset to make it robust against potential outliers in the future if ever there are any in the future, possibly when the dataset grows larger. By capping the data at specified percentiles we reduce the possible influed of these outliers. \n",
    "\n",
    "We also provided the before and after outlier treatment visualizations to identify the outliers (if there are any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to check for outliers\n",
    "outlier_columns = ['current_minute', 'home_yellow_cards', 'away_yellow_cards', 'home_on_target',\n",
    "                   'away_on_target', 'home_attacks', 'away_attacks', 'home_dangerous_attacks',\n",
    "                   'away_dangerous_attacks', 'home_corners', 'away_corners', 'home_off_target',\n",
    "                   'away_off_target', 'home_possession', 'away_possession']\n",
    "\n",
    "def winsorize(data, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Winsorizes data by capping values at specified percentiles.\"\"\"\n",
    "    lower_bound = np.percentile(data, lower_percentile)\n",
    "    upper_bound = np.percentile(data, upper_percentile)\n",
    "    return np.clip(data, lower_bound, upper_bound)\n",
    "\n",
    "def truncate(data, upper_bound):\n",
    "    \"\"\"Truncates data by removing values exceeding the upper bound.\"\"\"\n",
    "    return data[data <= upper_bound]\n",
    "\n",
    "def detect_possession_outliers(data, method='zscore', z_score_threshold=3, lower_bound=20, upper_bound=80):\n",
    "    \"\"\"\n",
    "    Identifies outliers in home and away possession data using different methods.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas DataFrame containing home_possession and away_possession columns.\n",
    "        method: The outlier detection method to use ('zscore', 'percentile', 'domain').\n",
    "        z_score_threshold: Threshold for Z-scores (used with 'zscore' method).\n",
    "        lower_bound: Lower threshold for possession values (used with 'domain' method).\n",
    "        upper_bound: Upper threshold for possession values (used with 'domain' method).\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with additional columns indicating potential outliers.\n",
    "    \"\"\"\n",
    "    home_possession = data['home_possession']\n",
    "    away_possession = data['away_possession']\n",
    "\n",
    "    if method == 'zscore':\n",
    "        # Z-score method (not used for normalization)\n",
    "        pass\n",
    "    elif method == 'percentile':\n",
    "        # Percentile-based method (not used for normalization)\n",
    "        pass\n",
    "    elif method == 'domain':\n",
    "        # Domain knowledge-based method\n",
    "        data['home_possession_outlier'] = np.where((home_possession < lower_bound) | (home_possession > upper_bound), True, False)\n",
    "        data['away_possession_outlier'] = np.where((away_possession < lower_bound) | (away_possession > upper_bound), True, False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid outlier detection method. Choose 'zscore', 'percentile', or 'domain'.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Handle outliers for other columns\n",
    "for col in outlier_columns:\n",
    "    if col == 'current_minute':\n",
    "        sports[col] = truncate(sports[col], 105)\n",
    "    elif col.endswith('_cards'):\n",
    "        sports[col] = winsorize(sports[col], upper_percentile=80)\n",
    "    else:\n",
    "        sports[col] = winsorize(sports[col])\n",
    "\n",
    "# Choose a method for possession outliers and apply it\n",
    "sports = detect_possession_outliers(sports.copy(), method='domain', lower_bound=20, upper_bound=80)  # Example: using domain-based method\n",
    "\n",
    "# Calculate goals scored after current minute\n",
    "def calculate_remaining_goals(row):\n",
    "    total_goals = row['home_score'] + row['away_score']\n",
    "    goals_so_far = total_goals - row['home_score'] - row['away_score']\n",
    "    return total_goals - goals_so_far\n",
    "\n",
    "sports['goals_remaining'] = sports.apply(calculate_remaining_goals, axis=1)\n",
    "\n",
    "# Calculate additional features\n",
    "epsilon = 1e-10  # Add a small number to avoid division by zero\n",
    "sports['home_attack_efficiency'] = sports['home_score'] / (sports['home_attacks'] + epsilon)\n",
    "sports['away_attack_efficiency'] = sports['away_score'] / (sports['away_attacks'] + epsilon)\n",
    "\n",
    "sports['home_shot_conversion_rate'] = sports['home_score'] / (sports['home_on_target'] + sports['home_off_target'] + epsilon)\n",
    "sports['away_shot_conversion_rate'] = sports['away_score'] / (sports['away_on_target'] + sports['away_off_target'] + epsilon)\n",
    "\n",
    "sports['possession_differential'] = sports['home_possession'] - sports['away_possession']\n",
    "\n",
    "# Select columns for normalization (excluding uuid)\n",
    "columns_to_normalize = [col for col in sports.columns if col != 'uuid']\n",
    "\n",
    "# Create a MinMaxScaler object (normalizes to -1 to 1 range)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Apply normalization to selected columns\n",
    "sports[columns_to_normalize] = scaler.fit_transform(sports[columns_to_normalize])\n",
    "\n",
    "# Visualize distributions after outlier treatment and normalization\n",
    "fig, axes = plt.subplots(nrows=len(outlier_columns) + 2, ncols=2, figsize=(15, 40))\n",
    "\n",
    "for i, col in enumerate(outlier_columns):\n",
    "    # Box plots\n",
    "    sns.boxplot(data=sports, x=col, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'{col} - Before Treatment')\n",
    "\n",
    "    sns.boxplot(data=sports, x=col, ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f'{col} - After Treatment')\n",
    "\n",
    "# Visualize possession data with outliers\n",
    "sns.boxplot(data=sports, x='home_possession', ax=axes[len(outlier_columns), 0], hue='home_possession_outlier')\n",
    "sns.boxplot(data=sports, x='away_possession', ax=axes[len(outlier_columns), 1], hue='away_possession_outlier')\n",
    "\n",
    "# Histograms for possession data with outliers\n",
    "sns.histplot(data=sports, x='home_possession', kde=True, ax=axes[len(outlier_columns) + 1, 0], hue='home_possession_outlier')\n",
    "sns.histplot(data=sports, x='away_possession', kde=True, ax=axes[len(outlier_columns) + 1, 1], hue='away_possession_outlier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did boxplots and histograms for all relevant variables and utilized truncation and winsorization. \n",
    "\n",
    "In our current case, there are no obvious outliers, because the initial data might not have contained extreme values that would significantly impact the results. There is also the idea of playstyle variations where the teams at different current_minutes would have varying possessions, attacks, or accuracy.\n",
    "\n",
    "Despite this, still applying outlier detection methods still offers a safety net from future outliners. We also applied normalization to bring the features to a common scale between 0 and 1, this will help us ensure that there is feature scaling and improved algorithm performance from algorithms like such as neural networks and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "`goals_remaining`: This is our target variable, representing the number of goals scored in the remaining time of the match.\n",
    "\n",
    "Ratio of Attack to Goals\n",
    "\n",
    "`home_attack_efficiency`: Measures the efficiency of the home team in converting attacks into goals.\n",
    "`away_attack_efficiency`: Measures the efficiency of the away team in converting attacks into goals.\n",
    "\n",
    "Ratio of Shots to Goals\n",
    "\n",
    "`home_shot_conversion_rate`: Measures the efficiency of the home team in converting shots into goals.\n",
    "`away_shot_conversion_rate`: Measures the efficiency of the away team in converting shots into goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate goals scored after current minute\n",
    "def calculate_remaining_goals(row):\n",
    "    total_goals = row['home_score'] + row['away_score']\n",
    "    goals_so_far = total_goals - row['home_score'] - row['away_score']\n",
    "    return total_goals - goals_so_far\n",
    "\n",
    "sports['goals_remaining'] = sports.apply(calculate_remaining_goals, axis=1)\n",
    "\n",
    "# Calculate additional features\n",
    "sports['home_attack_efficiency'] = sports['home_score'] / sports['home_attacks']\n",
    "sports['away_attack_efficiency'] = sports['away_score'] / sports['away_attacks']\n",
    "sports['home_shot_conversion_rate'] = sports['home_score'] / (sports['home_on_target'] + sports['home_off_target'])\n",
    "sports['away_shot_conversion_rate'] = sports['away_score'] / (sports['away_on_target'] + sports['away_off_target'])\n",
    "\n",
    "print(sports['goals_remaining'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Features**\n",
    "\n",
    "The uuid column is a unique identifier that does not provide any predictive information, therefore it's correct to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant column\n",
    "sports = sports.drop('uuid', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **correlation matrix** is a table showing the correlation coefficients between many variables. Each entry in the table shows the correlation between two variables. A correlation coefficient is a statistical measure that calculates the strength of the relationship between the relative movements of the two variables. The range of values for the correlation coefficient is -1.0 to 1.0. If a correlation is close to 1, it indicates a strong positive relationship between the variables. When that number is close to -1, the variables have a strong negative relationship.\n",
    "\n",
    "In the given heatmap:\n",
    "\n",
    "- Dark blue cells indicate a strong positive correlation, dark red cells indicate a strong negative correlation, and white cells indicate no correlation.\n",
    "\n",
    "- For example, 'home_score' and 'home_attack_efficiency' have a strong positive correlation. This suggests that as the home team's score increases, their attack efficiency also tends to increase.\n",
    "\n",
    "- Conversely, 'away_possession_outlier' and 'home_attack_efficiency' have a strong negative correlation, shown by a darker red shade and numerical value close to -1. This suggests that when the away team's possession is considered an outlier (either extremely high or low), the home team's attack efficiency tends to be lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = sports.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title('Correlation Matrix of Sports DataFrame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to remove the card-related variables as they might not have a direct impact on specific metrics, and are focused on which specific players do get these cards, which is data not present. Attack efficiency and ratios of goals to shots has more of a direct influence to the number of goals.\n",
    "\n",
    "If the dataset did have the extra data for individual players, we believe that focusing more on the attack-related variables would give us a clearer answer as these unnecessary variables can add noise into our model and possibly lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['home_yellow_cards', 'away_yellow_cards', 'home_red_cards', 'away_red_cards']\n",
    "sports = sports.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA is a critical step in any data analysis or machine learning project. It allows us to understand the data’s characteristics, identify patterns and relationships, detect anomalies, inform feature engineering, and check assumptions. For instance, EDA on the ‘Sports’ dataset will provide insights into the factors influencing the number of goals scored during the remaining playtime. These insights will guide our choice of regression model and help us engineer features that improve our model’s predictive performance. \n",
    "\n",
    "We can try to look at the average goals remaining for all the games, to have a general idea of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_goals_remaining = sports['goals_remaining'].mean()\n",
    "\n",
    "print(f\"Average Goals Remaining: {average_goals_remaining}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will generate a history plot of the goals_remaining variable to visualize its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(sports['goals_remaining'], kde=True)\n",
    "plt.title('Distribution of Goals Remaining')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right-skewness for goals remaining indicates that while most matches have fewer goals remaining, there are some matches where a significantly higher number of goals are scored towards the end. This distribution suggests that matches can vary widely in the number of goals scored in the remaining time.\n",
    "\n",
    "Below we can first visualize the home attack efficiency and shot conversion rate, here in the Kernel Density Estimate (KDE), it's basically a smoothed out version of the histogram so we can better see the distribution and probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sports['home_attack_efficiency'], kde=True)\n",
    "plt.title('Distribution of Home Attack Efficiency')\n",
    "plt.xlim(0, 1)  \n",
    "plt.show()\n",
    "\n",
    "sns.histplot(sports['home_shot_conversion_rate'], kde=True)\n",
    "plt.title('Distribution of Home Shot Conversion Rate')\n",
    "plt.xlim(0, 1) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can generate the away attack efficiency and away shot conversion rate. We see a significant difference in their KDE as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sports['away_attack_efficiency'], kde=True)\n",
    "plt.title('Distribution of Away Attack Efficiency')\n",
    "plt.xlim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "sns.histplot(sports['away_shot_conversion_rate'], kde=True)\n",
    "plt.title('Distribution of Away Shot Conversion Rate')\n",
    "plt.xlim(0, 1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features attack_efficiency and shot_conversion_rate for both teams are right-skewed. This means that most teams have lower values for these features, indicating that they are generally less efficient in converting attacks into goals. However, there are a few teams with very high values, suggesting that only a small number of teams achieve high efficiency in these areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='home_attack_efficiency', y='goals_remaining', data=sports)\n",
    "plt.title('Home Attack Efficiency vs. Goals Remaining')\n",
    "plt.xlabel('Home Attack Efficiency')\n",
    "plt.ylabel('Goals Remaining')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='home_shot_conversion_rate', y='goals_remaining', data=sports)\n",
    "plt.title('Home Shot Conversion Rate vs. Goals Remaining')\n",
    "plt.xlabel('Shot Conversion Rate')\n",
    "plt.ylabel('Goals Remaining')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goals remaining vary despite most teams having a low attack efficiency means that this variabile could be due to other factors influencing the number of goals remaining that are not captured by attack efficiency and shot conversion alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='away_attack_efficiency', y='goals_remaining', data=sports)\n",
    "plt.title('Away Attack Efficiency vs. Goals Remaining')\n",
    "plt.xlabel('Home Attack Efficiency')\n",
    "plt.ylabel('Goals Remaining')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.scatterplot(x='away_shot_conversion_rate', y='goals_remaining', data=sports)\n",
    "plt.title('Away Shot Conversion Rate vs. Goals Remaining')\n",
    "plt.xlabel('Shot Conversion Rate')\n",
    "plt.ylabel('Goals Remaining')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same interpretation can be concluded with the away attack efficiency, but it now differs with the away shot conversion rate where the points are much more scattered but is still mostly right-skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training is a crucial step in the machine learning pipeline. It involves learning the underlying patterns in the training data so that we can make predictions on unseen data. The goal is to find a model that generalizes well, meaning it accurately predicts the outcomes on new, unseen data based on the patterns it learned from the training data.\n",
    "\n",
    "In this project, we trained several types of regression models on our 'Sports' dataset:\n",
    "\n",
    "1. **Linear Regression**: This is a simple and commonly used type of regression that assumes a linear relationship between the input variables (x) and the single output variable (y). It can be used when the relationship between the input and output variables is approximately linear.\n",
    "\n",
    "2. **Regression Trees (Decision Trees)**: This is a type of model that breaks down our dataset into smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes, which provide a clear interpretation of why the model is making certain predictions.\n",
    "\n",
    "3. **Polynomial Regression**: This is a type of regression that models the relationship between the input variable (x) and the output variable (y) as an nth degree polynomial. Polynomial regression can model relationships between variables that aren't linear and can fit data with curves or slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using untruncated data\n",
    "# with open('sports.csv', 'r') as csv_file:\n",
    "#     raw_data = csv.reader(csv_file)\n",
    "#     next(csv_file)\n",
    "#     X_sports = np.empty((0, 1), int)\n",
    "#     y_sports = np.empty((0, 1), int)\n",
    "#     for row in raw_data:\n",
    "#         X_sports = np.vstack([X_sports, np.array([int(row[8])])])  # column for home attacks\n",
    "#         y_sports = np.append(y_sports, np.array([int(row[10])]))    # column for home dangerous attacks\n",
    "\n",
    "# Using truncated/cleaned data\n",
    "X_sports = sports['home_attacks'].values.reshape(-1, 1)\n",
    "y_sports = sports['home_dangerous_attacks'].values\n",
    "\n",
    "print('Training data shape:', X_sports.shape)\n",
    "print('Ground truth values shape:', y_sports.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_sports, y_sports, 'ro', label='Original data')\n",
    "\n",
    "plt.ylabel('Home dangerous attacks')\n",
    "plt.xlabel('Home attacks')\n",
    "ax.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For truncated/cleaned data\n",
    "# model = SGDRegressor(eta0=1e-9, max_iter=200, penalty=None, learning_rate='constant', random_state=1, verbose=1)\n",
    "\n",
    "# For untruncated data\n",
    "model = SGDRegressor(eta0=1e-9, max_iter=200, learning_rate='optimal', random_state=1, verbose=1, alpha=0.001)\n",
    "\n",
    "model.fit(X_sports, y_sports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the predicted linear regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the model\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_sports, y_sports, 'ro', label='Original data')\n",
    "\n",
    "line, = ax.plot(X_sports, model.predict(X_sports), label='Regression line')\n",
    "\n",
    "ax.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the y value to the target variable and X to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = sports[['home_attack_efficiency',\n",
    "            'away_attack_efficiency', \n",
    "            'home_shot_conversion_rate', \n",
    "            'away_shot_conversion_rate']]\n",
    "\n",
    "y = sports['goals_remaining']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(y_test, y_pred, color='red', label='Predicted') \n",
    "plt.scatter(y_test, y_test, color='blue', label='Actual')\n",
    "\n",
    "plt.xlabel('Actual Goals Remaining')\n",
    "plt.ylabel('Predicted Goals Remaining')\n",
    "plt.title('Actual vs Predicted Goals Remaining')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
