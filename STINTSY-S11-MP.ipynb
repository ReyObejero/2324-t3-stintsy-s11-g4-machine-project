{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of Members:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __APA, GIUSIPPI MARIA II DEL ROSARIO__\n",
    "- __OBEJERO, REY FERBS MAGALLON__\n",
    "- __RAMIREZ, BENMAR SIM GREFALDA__\n",
    "- __RAMOS, RONN PATRICK BICERA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we chose for this machine project is the \"Sports\" dataset. \n",
    "\n",
    "Our project will have us go through the process of selecting a dataset, describing it, performing EDA, data preprocessing and cleaning, model training, HP tuning, model selection, and extracting insights from the data.\n",
    "\n",
    "We aim to leverage the data from the soccer matches to preduct the number of goals that will be scored during the remaining playtime. The target of this project is regression as we aim to predict a continous outcome, which is the **number of goals to be scored** during the remaining time.\n",
    "\n",
    "We will implement the use of machine learning and other techniques taught in class to build our predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is related to soccer matches and will be used to predict how many goals will be scored during the remaining match time. The dataset contains 10,000 such snapshots, each taken at a different, random point in time during a match. \n",
    "\n",
    "In the context of this dataset, a “snapshot” refers to a set of data points captured at a specific moment during a soccer match.\n",
    "\n",
    "As for the collection process, it can be inferred that the data might have been collected in real-time during this soccer match, capturing various events and statistics at different timestamps. This data would allow for more accurate predictions of future events based on the current state of the match, and whether it is best to bet on over or under a certain number of goals.\n",
    "\n",
    "There are only 7000 instances(rows) and 21 features(columns) in the dataset. Each of the features are statistics of both events for the home and away team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Variable Name** | **Description**|\n",
    "|--------------------------------------|----------------|\n",
    "|**uuid** | Snapshot’s unique ID|\n",
    "|**current_minute** | Match’s current minute, including 15 min half-time|\n",
    "|**home_score and away_score** | Home and away team’s goals.|\n",
    "|**home_yellow_cards and away_yellow_cards**| Home and away team’s yellow cards.|\n",
    "|**home_red_cards and away_red_cards**| Home and away team’s red cards.|\n",
    "|**home_attacks and away_attacks**| Home and away team’s attempted attacks.|\n",
    "|**home_dangerous_attacks and away_dangerous_attacks**| Home and away team’s dangerous attacks.|\n",
    "|**home_corners and away_corners**| Home and away team’s awarded corners.|\n",
    "|**home_off_target and away_off_target**| Home and away team’s off-target shots.|\n",
    "|**home_on_target and away_on_target**| Home and away team’s on-target shots.|\n",
    "|**home_possession and away_possession** |Home and away team’s ball possession %.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this machine project, we will utilize the following Python libraries:\n",
    "\n",
    "- `one`:\n",
    "- `two`:\n",
    "- `three`:\n",
    "- `four`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'linear_regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlinear_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalyticalMethod, compute_RMSE\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlinear_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolyFitMethod\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'linear_regression'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import h5py\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from linear_regression import AnalyticalMethod, compute_RMSE\n",
    "from linear_regression import PolyFitMethod\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from neural_network import NeuralNetwork\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import log_loss, accuracy_score, classification_report, ParameterGrid\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now be performing various methods of preprocessing and data cleaning to enhance the dataset's usability and suitability to our exploratory analysis and model training.\n",
    "\n",
    "This is also to ensure that the dataset will be ready for use, which means we will be checking for missing values, duplicates, outliers and other errors. Making it so that the data is in its correct representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>current_minute</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>home_yellow_cards</th>\n",
       "      <th>away_yellow_cards</th>\n",
       "      <th>home_red_cards</th>\n",
       "      <th>away_red_cards</th>\n",
       "      <th>home_attacks</th>\n",
       "      <th>away_attacks</th>\n",
       "      <th>...</th>\n",
       "      <th>away_dangerous_attacks</th>\n",
       "      <th>home_corners</th>\n",
       "      <th>away_corners</th>\n",
       "      <th>home_off_target</th>\n",
       "      <th>away_off_target</th>\n",
       "      <th>home_on_target</th>\n",
       "      <th>away_on_target</th>\n",
       "      <th>home_possession</th>\n",
       "      <th>away_possession</th>\n",
       "      <th>final_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2f9dd904-8d9d-4aa0-a9d9-4e0f3e50d742</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>869c1420-9153-4d73-8d79-5db31a8a80a4</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>549e25a3-4c93-459d-b115-fb303273c78a</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287b4be5-9163-4e2e-895d-d11052a97410</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a607da6a-d6b2-4652-b7a5-6c8e4b61c2b8</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid  current_minute  home_score  \\\n",
       "0  2f9dd904-8d9d-4aa0-a9d9-4e0f3e50d742              94           1   \n",
       "1  869c1420-9153-4d73-8d79-5db31a8a80a4              91           2   \n",
       "2  549e25a3-4c93-459d-b115-fb303273c78a               3           0   \n",
       "3  287b4be5-9163-4e2e-895d-d11052a97410              17           0   \n",
       "4  a607da6a-d6b2-4652-b7a5-6c8e4b61c2b8              76           1   \n",
       "\n",
       "   away_score  home_yellow_cards  away_yellow_cards  home_red_cards  \\\n",
       "0           2                  0                  0               0   \n",
       "1           0                  0                  0               0   \n",
       "2           0                  0                  0               0   \n",
       "3           0                  1                  0               0   \n",
       "4           0                  1                  0               0   \n",
       "\n",
       "   away_red_cards  home_attacks  away_attacks  ...  away_dangerous_attacks  \\\n",
       "0               0            39            45  ...                      26   \n",
       "1               0             0             0  ...                       0   \n",
       "2               0             3             1  ...                       0   \n",
       "3               0            41            46  ...                      25   \n",
       "4               0            69            78  ...                      46   \n",
       "\n",
       "   home_corners  away_corners  home_off_target  away_off_target  \\\n",
       "0             4             1                4                0   \n",
       "1             0             0                0                0   \n",
       "2             0             0                0                0   \n",
       "3             1             0                0                2   \n",
       "4             1             1                0                3   \n",
       "\n",
       "   home_on_target  away_on_target  home_possession  away_possession  \\\n",
       "0               4               6               48               52   \n",
       "1               0               0               50               50   \n",
       "2               0               0               62               38   \n",
       "3               0               0               55               45   \n",
       "4               4               4               45               55   \n",
       "\n",
       "   final_delta  \n",
       "0            1  \n",
       "1            1  \n",
       "2            4  \n",
       "3            2  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing and reading the Sports.csv\n",
    "sports = pd.read_csv(\"sports.csv\")\n",
    "sports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Missing Values**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check each column for missing values\n",
    "missing_values = sports.isnull().sum()\n",
    "\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Duplicates**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Entries:\n",
      " Empty DataFrame\n",
      "Columns: [uuid, current_minute, home_score, away_score, home_yellow_cards, away_yellow_cards, home_red_cards, away_red_cards, home_attacks, away_attacks, home_dangerous_attacks, away_dangerous_attacks, home_corners, away_corners, home_off_target, away_off_target, home_on_target, away_on_target, home_possession, away_possession, final_delta]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n",
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "# Find the duplicates\n",
    "duplicate_uuids = sports.duplicated(keep=False)\n",
    "\n",
    "# Create a dataframe for duplicates\n",
    "duplicates = sports[duplicate_uuids]\n",
    "\n",
    "# Display the rows with duplicates\n",
    "print(\"Duplicate Entries:\\n\", duplicates)\n",
    "\n",
    "# Removing the duplicates if detected in dataframe\n",
    "if not duplicates.empty:\n",
    "    sports = sports.drop_duplicates(keep='first')\n",
    "    print(\"Duplicates removed. Dataset has {sports.shape[0]} rows.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Cleaning Processes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found inconsistencies in possession data:\n",
      "      home_possession  away_possession\n",
      "72                 67               32\n",
      "881                23               76\n",
      "1084               77               22\n",
      "1185               75               24\n",
      "1421               33               66\n",
      "1485               77               22\n",
      "1950               71               28\n",
      "1984               83               16\n",
      "2415               30               69\n",
      "2849               27               72\n",
      "2895               78               21\n",
      "2953               79               20\n",
      "3222               31               68\n",
      "3313               65               34\n",
      "4065               25               74\n",
      "4669               73               26\n",
      "5302               78               21\n",
      "5509               25               74\n",
      "6299               73               26\n",
      "6527               33               66\n",
      "6660               72               27\n",
      "6839               76               23\n"
     ]
    }
   ],
   "source": [
    "# Replacing any erroneous infinite values\n",
    "sports = sports.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Dropping columns with more than 50% missing values\n",
    "sports = sports.dropna(thresh=len(sports)*0.5, axis=1)\n",
    "\n",
    "# Check for incorrect possession percentages\n",
    "incorrect_possession_indices = sports[~(sports['home_possession'] + sports['away_possession'] == 100)].index\n",
    "\n",
    "if len(incorrect_possession_indices) > 0:\n",
    "    print(\"Found inconsistencies in possession data:\")\n",
    "    print(sports.loc[incorrect_possession_indices, ['home_possession', 'away_possession']])\n",
    "else:\n",
    "    print(\"Possession data is consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are percentages that don't equate to 100, but the difference is neglible because it is only from 99 to 100 which won't cause much noise in the analysis. It isn't exactly necessary to remove it at this point of the dataset but is also another consideration we took to prepare the if ever the dataset grew larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Type Checking**\n",
    "\n",
    "Having appropriate data types for the values in the dataset is crucial to data analysis and model training. Luckily for us, the dataset we currently have has the appropriate values as all of the variables and their values are appropriate to their data type.\n",
    "\n",
    "The importance of having correct data types is that we have the following:\n",
    "\n",
    "Data Integrity,\n",
    "Accuracy,\n",
    "Efficiency,\n",
    "Compatibility, \n",
    "and Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data types present:\n",
      " uuid                      object\n",
      "current_minute             int64\n",
      "home_score                 int64\n",
      "away_score                 int64\n",
      "home_yellow_cards          int64\n",
      "away_yellow_cards          int64\n",
      "home_red_cards             int64\n",
      "away_red_cards             int64\n",
      "home_attacks               int64\n",
      "away_attacks               int64\n",
      "home_dangerous_attacks     int64\n",
      "away_dangerous_attacks     int64\n",
      "home_corners               int64\n",
      "away_corners               int64\n",
      "home_off_target            int64\n",
      "away_off_target            int64\n",
      "home_on_target             int64\n",
      "away_on_target             int64\n",
      "home_possession            int64\n",
      "away_possession            int64\n",
      "final_delta                int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Printing all of the present data types\n",
    "print(\"All data types present:\\n\", sports.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and Treatment\n",
    "\n",
    "Outliers can significantly impact statistical measures and model performance. In the context of our dataset, outliers might represent unusual game conditions, errors in data collection, or truly exceptional events. \n",
    "\n",
    "Surprisingly enough, there are no major outliers in our dataset, because in the dataset, varying values like attacks, cards, and accuracy are all subject to the team's playstyle and ability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we performed winsorization on the dataset to make it robust against potential outliers in the future if ever there are any in the future, possibly when the dataset grows larger. By capping the data at specified percentiles we reduce the possible influed of these outliers. \n",
    "\n",
    "We also provided the before and after outlier treatment visualizations to identify the outliers (if there are any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   uuid  current_minute  home_score  \\\n",
      "0  2f9dd904-8d9d-4aa0-a9d9-4e0f3e50d742              94           1   \n",
      "1  869c1420-9153-4d73-8d79-5db31a8a80a4              91           2   \n",
      "2  549e25a3-4c93-459d-b115-fb303273c78a               3           0   \n",
      "3  287b4be5-9163-4e2e-895d-d11052a97410              17           0   \n",
      "4  a607da6a-d6b2-4652-b7a5-6c8e4b61c2b8              76           1   \n",
      "\n",
      "   away_score  home_yellow_cards  away_yellow_cards  home_red_cards  \\\n",
      "0           2                  0                  0               0   \n",
      "1           0                  0                  0               0   \n",
      "2           0                  0                  0               0   \n",
      "3           0                  1                  0               0   \n",
      "4           0                  1                  0               0   \n",
      "\n",
      "   away_red_cards  home_attacks  away_attacks  ...  away_dangerous_attacks  \\\n",
      "0               0            39            45  ...                      26   \n",
      "1               0             0             0  ...                       0   \n",
      "2               0             3             1  ...                       0   \n",
      "3               0            41            46  ...                      25   \n",
      "4               0            69            78  ...                      46   \n",
      "\n",
      "   home_corners  away_corners  home_off_target  away_off_target  \\\n",
      "0             4             1                4                0   \n",
      "1             0             0                0                0   \n",
      "2             0             0                0                0   \n",
      "3             1             0                0                2   \n",
      "4             1             1                0                3   \n",
      "\n",
      "   home_on_target  away_on_target  home_possession  away_possession  \\\n",
      "0               4               6               48               52   \n",
      "1               0               0               50               50   \n",
      "2               0               0               62               38   \n",
      "3               0               0               55               45   \n",
      "4               4               4               45               55   \n",
      "\n",
      "   final_delta  \n",
      "0            1  \n",
      "1            1  \n",
      "2            4  \n",
      "3            2  \n",
      "4            0  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sports.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Apply normalization to selected columns\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m sports[columns_to_normalize] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msports\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns_to_normalize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Visualize distributions after outlier treatment and normalization\u001b[39;00m\n\u001b[0;32m    101\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(outlier_columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m40\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:450\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:490\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    487\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    489\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_pass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m data_min \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    498\u001b[0m data_max \u001b[38;5;241m=\u001b[39m _array_api\u001b[38;5;241m.\u001b[39m_nanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1059\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1059\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sim Ramirez\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load data from CSV\n",
    "sports = pd.read_csv('sports.csv')\n",
    "\n",
    "# Columns to check for outliers\n",
    "outlier_columns = ['current_minute', 'home_yellow_cards', 'away_yellow_cards', 'home_on_target',\n",
    "                   'away_on_target', 'home_attacks', 'away_attacks', 'home_dangerous_attacks',\n",
    "                   'away_dangerous_attacks', 'home_corners', 'away_corners', 'home_off_target',\n",
    "                   'away_off_target', 'home_possession', 'away_possession']\n",
    "\n",
    "def winsorize(data, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"Winsorizes data by capping values at specified percentiles.\"\"\"\n",
    "    lower_bound = np.percentile(data, lower_percentile)\n",
    "    upper_bound = np.percentile(data, upper_percentile)\n",
    "    return np.clip(data, lower_bound, upper_bound)\n",
    "\n",
    "def truncate(data, upper_bound):\n",
    "    \"\"\"Truncates data by removing values exceeding the upper bound.\"\"\"\n",
    "    return data[data <= upper_bound]\n",
    "\n",
    "def detect_possession_outliers(data, method='zscore', z_score_threshold=3, lower_bound=20, upper_bound=80):\n",
    "    \"\"\"\n",
    "    Identifies outliers in home and away possession data using different methods.\n",
    "\n",
    "    Args:\n",
    "        data: Pandas DataFrame containing home_possession and away_possession columns.\n",
    "        method: The outlier detection method to use ('zscore', 'percentile', 'domain').\n",
    "        z_score_threshold: Threshold for Z-scores (used with 'zscore' method).\n",
    "        lower_bound: Lower threshold for possession values (used with 'domain' method).\n",
    "        upper_bound: Upper threshold for possession values (used with 'domain' method).\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with additional columns indicating potential outliers.\n",
    "    \"\"\"\n",
    "    home_possession = data['home_possession']\n",
    "    away_possession = data['away_possession']\n",
    "\n",
    "    if method == 'zscore':\n",
    "        # Z-score method (not used for normalization)\n",
    "        pass\n",
    "    elif method == 'percentile':\n",
    "        # Percentile-based method (not used for normalization)\n",
    "        pass\n",
    "    elif method == 'domain':\n",
    "        # Domain knowledge-based method\n",
    "        data['home_possession_outlier'] = np.where((home_possession < lower_bound) | (home_possession > upper_bound), True, False)\n",
    "        data['away_possession_outlier'] = np.where((away_possession < lower_bound) | (away_possession > upper_bound), True, False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid outlier detection method. Choose 'zscore', 'percentile', or 'domain'.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Handle outliers for other columns\n",
    "for col in outlier_columns:\n",
    "    if col == 'current_minute':\n",
    "        sports[col] = truncate(sports[col], 105)\n",
    "    elif col.endswith('_cards'):\n",
    "        sports[col] = winsorize(sports[col], upper_percentile=80)\n",
    "    else:\n",
    "        sports[col] = winsorize(sports[col])\n",
    "\n",
    "# Choose a method for possession outliers and apply it\n",
    "sports = detect_possession_outliers(sports.copy(), method='domain', lower_bound=20, upper_bound=80)  # Example: using domain-based method\n",
    "\n",
    "# Calculate goals scored after current minute\n",
    "def calculate_remaining_goals(row):\n",
    "    total_goals = row['home_score'] + row['away_score']\n",
    "    goals_so_far = total_goals - row['home_score'] - row['away_score']\n",
    "    return total_goals - goals_so_far\n",
    "\n",
    "sports['goals_remaining'] = sports.apply(calculate_remaining_goals, axis=1)\n",
    "\n",
    "# Calculate additional features\n",
    "sports['home_attack_efficiency'] = sports['home_score'] / sports['home_attacks']\n",
    "sports['away_attack_efficiency'] = sports['away_score'] / sports['away_attacks']\n",
    "\n",
    "sports['home_shot_conversion_rate'] = sports['home_score'] / (sports['home_on_target'] + sports['home_off_target'])\n",
    "sports['away_shot_conversion_rate'] = sports['away_score'] / (sports['away_on_target'] + sports['away_off_target'])\n",
    "\n",
    "sports['possession_differential'] = sports['home_possession'] - sports['away_possession']\n",
    "\n",
    "# Handle potential division by zero in attack efficiency\n",
    "sports['home_attack_efficiency'] = sports['home_attack_efficiency'].replace([np.inf, -np.inf], 0)\n",
    "sports['away_attack_efficiency'] = sports['away_attack_efficiency'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Select columns for normalization (excluding uuid)\n",
    "columns_to_normalize = [col for col in sports.columns if col != 'uuid']\n",
    "\n",
    "# Create a MinMaxScaler object (normalizes to -1 to 1 range)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Apply normalization to selected columns\n",
    "sports[columns_to_normalize] = scaler.fit_transform(sports[columns_to_normalize])\n",
    "\n",
    "# Visualize distributions after outlier treatment and normalization\n",
    "fig, axes = plt.subplots(nrows=len(outlier_columns) + 2, ncols=2, figsize=(15, 40))\n",
    "\n",
    "for i, col in enumerate(outlier_columns):\n",
    "    # Box plots\n",
    "    sns.boxplot(data=sports, x=col, ax=axes[i, 0])\n",
    "    axes[i, 0].set_title(f'{col} - Before Treatment')\n",
    "\n",
    "    sns.boxplot(data=sports, x=col, ax=axes[i, 1])\n",
    "    axes[i, 1].set_title(f'{col} - After Treatment')\n",
    "\n",
    "# Visualize possession data with outliers\n",
    "sns.boxplot(data=sports, x='home_possession', ax=axes[len(outlier_columns), 0], hue='home_possession_outlier')\n",
    "sns.boxplot(data=sports, x='away_possession', ax=axes[len(outlier_columns), 1], hue='away_possession_outlier')\n",
    "\n",
    "# Histograms for possession data with outliers\n",
    "sns.histplot(data=sports, x='home_possession', kde=True, ax=axes[len(outlier_columns) + 1, 0], hue='home_possession_outlier')\n",
    "sns.histplot(data=sports, x='away_possession', kde=True, ax=axes[len(outlier_columns) + 1, 1], hue='away_possession_outlier')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   uuid  current_minute  home_score  \\\n",
      "0  2f9dd904-8d9d-4aa0-a9d9-4e0f3e50d742            94.0           1   \n",
      "1  869c1420-9153-4d73-8d79-5db31a8a80a4            91.0           2   \n",
      "2  549e25a3-4c93-459d-b115-fb303273c78a             3.0           0   \n",
      "3  287b4be5-9163-4e2e-895d-d11052a97410            17.0           0   \n",
      "4  a607da6a-d6b2-4652-b7a5-6c8e4b61c2b8            76.0           1   \n",
      "\n",
      "   away_score  home_yellow_cards  away_yellow_cards  home_red_cards  \\\n",
      "0           2                  0                  0               0   \n",
      "1           0                  0                  0               0   \n",
      "2           0                  0                  0               0   \n",
      "3           0                  1                  0               0   \n",
      "4           0                  1                  0               0   \n",
      "\n",
      "   away_red_cards  home_attacks  away_attacks  ...  away_possession  \\\n",
      "0               0            39            45  ...               52   \n",
      "1               0             0             0  ...               50   \n",
      "2               0             3             1  ...               38   \n",
      "3               0            41            46  ...               45   \n",
      "4               0            69            78  ...               55   \n",
      "\n",
      "   final_delta  home_possession_outlier  away_possession_outlier  \\\n",
      "0            1                    False                    False   \n",
      "1            1                    False                    False   \n",
      "2            4                    False                    False   \n",
      "3            2                    False                    False   \n",
      "4            0                    False                    False   \n",
      "\n",
      "   goals_remaining  home_attack_efficiency  away_attack_efficiency  \\\n",
      "0                3                0.025641                0.044444   \n",
      "1                2                     inf                     NaN   \n",
      "2                0                0.000000                0.000000   \n",
      "3                0                0.000000                0.000000   \n",
      "4                1                0.014493                0.000000   \n",
      "\n",
      "   home_shot_conversion_rate  away_shot_conversion_rate  \\\n",
      "0                      0.125                   0.333333   \n",
      "1                        inf                        NaN   \n",
      "2                        NaN                        NaN   \n",
      "3                        NaN                   0.000000   \n",
      "4                      0.250                   0.000000   \n",
      "\n",
      "   possession_differential  \n",
      "0                       -4  \n",
      "1                        0  \n",
      "2                       24  \n",
      "3                       10  \n",
      "4                      -10  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did boxplots and histograms for all relevant variables and utilized truncation and winsorization. \n",
    "\n",
    "In our current case, there are no obvious outliers, because the initial data might not have contained extreme values that would significantly impact the results. There is also the idea of playstyle variations where the teams at different current_minutes would have varying possessions, attacks, or accuracy.\n",
    "\n",
    "Despite this, still applying outlier detection methods still offers a safety net from future outliners. We also applied standardization only to the numerical columns using StandardScaler to bring the features to a common scale, this will help us ensure that there is feature scaling and improved algorithm performance from algorithms like K-means, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**\n",
    "`goals_remaining`: This is our target variable, representing the number of goals scored in the remaining time of the match.\n",
    "\n",
    "Ratio of Attack to Goals\n",
    "\n",
    "`home_attack_efficiency`: Measures the efficiency of the home team in converting attacks into goals.\n",
    "`away_attack_efficiency`: Measures the efficiency of the away team in converting attacks into goals.\n",
    "\n",
    "Ratio of Shots to Goals\n",
    "\n",
    "`home_shot_conversion_rate`: Measures the efficiency of the home team in converting shots into goals.\n",
    "`away_shot_conversion_rate`: Measures the efficiency of the away team in converting shots into goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   uuid  current_minute  home_score  \\\n",
      "0  2f9dd904-8d9d-4aa0-a9d9-4e0f3e50d742        1.069318    0.190578   \n",
      "1  869c1420-9153-4d73-8d79-5db31a8a80a4        0.980413    1.124785   \n",
      "2  549e25a3-4c93-459d-b115-fb303273c78a       -1.627464   -0.743629   \n",
      "3  287b4be5-9163-4e2e-895d-d11052a97410       -1.212574   -0.743629   \n",
      "4  a607da6a-d6b2-4652-b7a5-6c8e4b61c2b8        0.535888    0.190578   \n",
      "\n",
      "   away_score  home_yellow_cards  away_yellow_cards  home_red_cards  \\\n",
      "0    1.491426          -0.686445          -0.722773       -0.095344   \n",
      "1   -0.674995          -0.686445          -0.722773       -0.095344   \n",
      "2   -0.674995          -0.686445          -0.722773       -0.095344   \n",
      "3   -0.674995           1.456781          -0.722773       -0.095344   \n",
      "4   -0.674995           1.456781          -0.722773       -0.095344   \n",
      "\n",
      "   away_red_cards  home_attacks  away_attacks  ...  home_possession  \\\n",
      "0       -0.125202     -0.191477      0.079814  ...        -0.400131   \n",
      "1       -0.125202     -1.341327     -1.348426  ...        -0.175058   \n",
      "2       -0.125202     -1.252877     -1.316687  ...         1.175377   \n",
      "3       -0.125202     -0.132511      0.111552  ...         0.387623   \n",
      "4       -0.125202      0.693023      1.127189  ...        -0.737739   \n",
      "\n",
      "   away_possession  final_delta  home_possession_outlier  \\\n",
      "0         0.400141    -0.293782                      0.0   \n",
      "1         0.175081    -0.293782                      0.0   \n",
      "2        -1.175281     1.820482                      0.0   \n",
      "3        -0.387570     0.410973                      0.0   \n",
      "4         0.737732    -0.998537                      0.0   \n",
      "\n",
      "   away_possession_outlier  goals_remaining  home_attack_efficiency  \\\n",
      "0                      0.0         1.682004               -0.995304   \n",
      "1                      0.0         0.449791               -0.838562   \n",
      "2                      0.0        -1.418624                0.593537   \n",
      "3                      0.0        -1.418624                5.611840   \n",
      "4                      0.0        -0.484417                0.274996   \n",
      "\n",
      "   away_attack_efficiency  home_shot_conversion_rate  \\\n",
      "0               18.686349                   0.148459   \n",
      "1                0.500580                  -0.548978   \n",
      "2                0.512646                   0.362946   \n",
      "3               -6.050926                   0.362946   \n",
      "4               -0.598830                  -1.365907   \n",
      "\n",
      "   away_shot_conversion_rate  \n",
      "0                   1.117822  \n",
      "1                   0.344053  \n",
      "2                   0.344053  \n",
      "3                   0.586768  \n",
      "4                  -0.464612  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate goals scored after current minute\n",
    "def calculate_remaining_goals(row):\n",
    "    total_goals = row['home_score'] + row['away_score']\n",
    "    goals_so_far = total_goals - row['home_score'] - row['away_score']\n",
    "    return total_goals - goals_so_far\n",
    "\n",
    "sports['goals_remaining'] = sports.apply(calculate_remaining_goals, axis=1)\n",
    "\n",
    "# Calculate additional features\n",
    "sports['home_attack_efficiency'] = sports['home_score'] / sports['home_attacks']\n",
    "sports['away_attack_efficiency'] = sports['away_score'] / sports['away_attacks']\n",
    "sports['home_shot_conversion_rate'] = sports['home_score'] / (sports['home_on_target'] + sports['home_off_target'])\n",
    "sports['away_shot_conversion_rate'] = sports['away_score'] / (sports['away_on_target'] + sports['away_off_target'])\n",
    "\n",
    "print(sports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Features**\n",
    "\n",
    "The uuid column is a unique identifier that does not provide any predictive information, therefore it's correct to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant column\n",
    "sports = sports.drop('uuid', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# set default size of plots\n",
    "plt.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
